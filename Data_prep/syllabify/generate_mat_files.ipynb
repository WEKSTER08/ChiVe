{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "from scipy.io import savemat\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = file.read().replace('\\n', ' ')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### words.mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_word_boundaries_data(tokenized_data):\n",
    "    words, start, end = [], [], []\n",
    "    for i in range(0, len(tokenized_data), 5):\n",
    "        start.append(float(tokenized_data[i+2]))\n",
    "        if i+2+5 < len(tokenized_data) and float(tokenized_data[i+2]) + float(tokenized_data[i+3]) > float(tokenized_data[i+2+5]):\n",
    "            end.append(float(tokenized_data[i+2+5]))\n",
    "        else:\n",
    "            end.append(float(tokenized_data[i+2]) + float(tokenized_data[i+3]))\n",
    "        words.append(tokenized_data[i+4])\n",
    "    return words, start, end\n",
    "    \n",
    "def save_words_mat(file_path, filename, save_directory):\n",
    "    data = read_file(file_path)\n",
    "    tokenized_data = data.replace('\\t', ' ').split(' ')\n",
    "    while(\"\" in tokenized_data):\n",
    "        tokenized_data.remove(\"\")\n",
    "\n",
    "    words, start, end = process_word_boundaries_data(tokenized_data)\n",
    "    word_time_intervals = list(zip(start, end))\n",
    "    print(word_time_intervals)\n",
    "    mat_data = {\"spurtWordTimes\": np.array(word_time_intervals, dtype=np.float_), \"words\": words}\n",
    "    mat_filename = save_directory + \"words/\" + filename[:-4] + \"_words.mat\"\n",
    "    print(mat_data)\n",
    "#     savemat(mat_filename, mat_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### vowel.mat & syllable.mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import syllabifier\n",
    "language = syllabifier.English # or: syllabifier.loadLanguage(\"english.cfg\")\n",
    "spn_containing_files = []\n",
    "PHONEME_LENGTH = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_phonemes(data_arr):\n",
    "    start, end, ph = [], [], []\n",
    "    final = ''\n",
    "    for i in range(0, len(data_arr), 3):\n",
    "        if data_arr[i+2] == \"sil\":\n",
    "            continue\n",
    "        if data_arr[i+2] == \"spn\":\n",
    "            spn_containing_files.append(filename)\n",
    "            print(filename)\n",
    "            return\n",
    "        start.append(data_arr[i])\n",
    "        end.append(data_arr[i+1])\n",
    "        ph.append(data_arr[i+2])\n",
    "        final += data_arr[i+2].upper() + ' '\n",
    "    return start, end, ph, final\n",
    "\n",
    "def syllabify_and_extract_times(language, final, start, end):\n",
    "    syllables = syllabifier.syllabify(language, final)\n",
    "    main_str = syllabifier.stringify(syllables)\n",
    "    arr = main_str.split(\".\")  # syllable array\n",
    "\n",
    "    final_st, final_end, syl = [], [], []\n",
    "    c = 0\n",
    "    for e in arr:\n",
    "        phoneme = e.split(\" \")\n",
    "        while \"\" in phoneme:\n",
    "            phoneme.remove(\"\")\n",
    "        final_st.append(start[c])\n",
    "        final_end.append(end[c+len(phoneme)-1])\n",
    "        syl.append(phoneme)\n",
    "        c += len(phoneme)\n",
    "    return final_st, final_end, syl\n",
    "\n",
    "def extract_vowels(start, end, ph):\n",
    "    v = syllabifier.English[\"vowels\"]\n",
    "    vowel_start, vowel_end = [], []\n",
    "    for i in range(len(ph)):\n",
    "        s = ph[i]\n",
    "        if len(ph[i]) == PHONEME_LENGTH:\n",
    "            s = ph[i][:-1]\n",
    "        if s.upper() in v:\n",
    "            vowel_start.append(float(start[i]))\n",
    "            vowel_end.append(float(end[i]))\n",
    "    return vowel_start, vowel_end\n",
    "\n",
    "def process_syllables(syl, start, end):\n",
    "    syllables = []\n",
    "    for s in syl:\n",
    "        ss = \" \".join([ph.lower()[:-1] if len(ph) == PHONEME_LENGTH else ph.lower() for ph in s])\n",
    "        if ss:\n",
    "            syllables.append(ss)\n",
    "    \n",
    "    syllable_times = [[float(start[i]), float(end[i])] for i in range(len(start))]\n",
    "    return syllables, syllable_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_vowel_and_syllable_mat(file_path, filename, save_directory):\n",
    "    data = read_file(file_path)\n",
    "    tokenized_data = data.replace('\\t', ' ').split(' ')\n",
    "    while \"\" in tokenized_data:\n",
    "        tokenized_data.remove(\"\")\n",
    "        \n",
    "    start, end, ph, final = process_phonemes(tokenized_data)\n",
    "    \n",
    "    final_st, final_end, syl = syllabify_and_extract_times(language, final, start, end)\n",
    "\n",
    "    vowel_start, vowel_end = extract_vowels(start, end, ph)\n",
    "    mdic_vowel = {\"vowelStartTime\": np.array(vowel_start, dtype=np.float32), \"vowelEndTime\": np.array(vowel_end, dtype=np.float32)}\n",
    "    savemat(\"{}vowel/{}_vowel.mat\".format(save_directory, filename[:-4]), mdic_vowel)\n",
    "\n",
    "    syllables, syllable_times = process_syllables(syl, final_st, final_end)\n",
    "    mdic_syllable = {\"spurtSyl\": np.array(syllables), \"spurtSylTimes\": np.array(syllable_times, dtype=np.float_)}\n",
    "    savemat(\"{}syllable/{}_syllable.mat\".format(save_directory, filename[:-4]), mdic_syllable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate mat files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# all these should end with a slash\n",
    "word_directory = \"../old_TCSSBC/word_and_phoneme_boundaries/DUC2001_FA_result/\"\n",
    "vowel_syl_directory = \"../old_TCSSBC/word_and_phoneme_boundaries/DUC2001_FA_result_phn/\"\n",
    "save_directory = \"./mat_files/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10707 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FT911-2650_62.txt\n",
      "[(0.03, 0.11), (0.11, 0.41), (0.41, 0.5599999999999999), (0.56, 1.09), (1.09, 1.18), (1.18, 1.27), (1.27, 1.67), (1.67, 1.7999999999999998), (1.8, 2.06), (2.06, 2.68), (2.68, 3.13), (3.13, 3.34), (3.34, 3.7199999999999998), (3.72, 4.09), (4.09, 4.27), (4.27, 4.3999999999999995), (4.4, 4.69), (4.69, 5.11), (5.11, 5.31), (5.31, 5.789999999999999), (5.79, 5.91), (5.91, 6.6), (6.6, 6.84), (6.84, 7.04), (7.04, 7.64)]\n",
      "{'spurtWordTimes': array([[0.03, 0.11],\n",
      "       [0.11, 0.41],\n",
      "       [0.41, 0.56],\n",
      "       [0.56, 1.09],\n",
      "       [1.09, 1.18],\n",
      "       [1.18, 1.27],\n",
      "       [1.27, 1.67],\n",
      "       [1.67, 1.8 ],\n",
      "       [1.8 , 2.06],\n",
      "       [2.06, 2.68],\n",
      "       [2.68, 3.13],\n",
      "       [3.13, 3.34],\n",
      "       [3.34, 3.72],\n",
      "       [3.72, 4.09],\n",
      "       [4.09, 4.27],\n",
      "       [4.27, 4.4 ],\n",
      "       [4.4 , 4.69],\n",
      "       [4.69, 5.11],\n",
      "       [5.11, 5.31],\n",
      "       [5.31, 5.79],\n",
      "       [5.79, 5.91],\n",
      "       [5.91, 6.6 ],\n",
      "       [6.6 , 6.84],\n",
      "       [6.84, 7.04],\n",
      "       [7.04, 7.64]]), 'words': ['the', 'ebrd', 'is', 'ministering', 'to', 'a', 'region', 'that', 'has', 'historical', 'ties', 'with', 'western', 'europe', 'and', 'that', 'has', 'declared', 'its', 'commitment', 'to', 'democracy', 'and', 'free', 'markets']}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "empty_files = [] # having 0 words\n",
    "for filename in tqdm(os.listdir(word_directory)):\n",
    "    if os.path.getsize(word_directory + filename) == 0:\n",
    "        empty_files.append(filename)\n",
    "\n",
    "    if filename.endswith(\".txt\") and os.path.getsize(word_directory + filename) > 0:\n",
    "        file_path = \"{}{}\".format(word_directory, filename)\n",
    "        print(filename)\n",
    "        save_words_mat(file_path, filename, save_directory)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10708/10708 [01:55<00:00, 92.70it/s] \n"
     ]
    }
   ],
   "source": [
    "for filename in tqdm(os.listdir(vowel_syl_directory)):\n",
    "    # word directory bcoz we check if there are no words\n",
    "    if filename.endswith(\".txt\") and os.path.getsize(word_directory + filename) > 0:\n",
    "        file_path = \"{}{}\".format(vowel_syl_directory, filename)\n",
    "        save_vowel_and_syllable_mat(file_path, filename, save_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['WSJ910208-0130_8.txt',\n",
       " 'LA103089-0043_46.txt',\n",
       " 'WSJ910710-0148_12.txt',\n",
       " 'SJMN91-06283083_37.txt',\n",
       " 'SJMN91-06283083_26.txt',\n",
       " 'WSJ911121-0136_15.txt',\n",
       " 'FBIS3-41_21.txt',\n",
       " 'LA061589-0143_29.txt',\n",
       " 'SJMN91-06246065_10.txt',\n",
       " 'LA010890-0031_72.txt',\n",
       " 'WSJ880621-0079_39.txt',\n",
       " 'LA052289-0050_90.txt',\n",
       " 'SJMN91-06246065_11.txt',\n",
       " 'LA102190-0045_61.txt',\n",
       " 'LA011889-0067_21.txt',\n",
       " 'LA081490-0030_18.txt',\n",
       " 'LA103089-0043_45.txt',\n",
       " 'SJMN91-06276078_14.txt',\n",
       " 'LA050889-0075_45.txt',\n",
       " 'LA061589-0143_8.txt',\n",
       " 'SJMN91-06187248_69.txt',\n",
       " 'LA103089-0043_67.txt',\n",
       " 'WSJ920114-0145_40.txt',\n",
       " 'LA102190-0045_62.txt',\n",
       " 'SJMN91-06246065_12.txt',\n",
       " 'LA103089-0043_63.txt',\n",
       " 'LA102190-0045_73.txt',\n",
       " 'LA032589-0044_33.txt',\n",
       " 'SJMN91-06283083_27.txt',\n",
       " 'LA042790-0205_18.txt',\n",
       " 'WSJ910702-0078_44.txt',\n",
       " 'WSJ910107-0139_40.txt',\n",
       " 'SJMN91-06246065_38.txt',\n",
       " 'LA061589-0143_7.txt',\n",
       " 'LA102190-0045_52.txt',\n",
       " 'LA081490-0030_19.txt',\n",
       " 'SJMN91-06290146_13.txt',\n",
       " 'WSJ880621-0079_70.txt',\n",
       " 'LA050889-0075_46.txt',\n",
       " 'WSJ911121-0136_13.txt',\n",
       " 'LA121189-0017_45.txt',\n",
       " 'WSJ890828-0011_30.txt',\n",
       " 'LA051590-0065_7.txt',\n",
       " 'LA121189-0017_44.txt',\n",
       " 'LA102190-0045_36.txt',\n",
       " 'LA052289-0050_65.txt',\n",
       " 'WSJ910702-0078_33.txt',\n",
       " 'SJMN91-06290146_12.txt',\n",
       " 'LA121189-0017_46.txt',\n",
       " 'SJMN91-06246065_34.txt',\n",
       " 'SJMN91-06187248_74.txt',\n",
       " 'SJMN91-06012224_20.txt',\n",
       " 'SJMN91-06184088_22.txt',\n",
       " 'LA121189-0017_41.txt',\n",
       " 'WSJ911121-0136_12.txt',\n",
       " 'LA050889-0075_47.txt',\n",
       " 'LA040689-0056_24.txt',\n",
       " 'LA102190-0045_55.txt',\n",
       " 'LA051590-0065_8.txt',\n",
       " 'LA032589-0044_34.txt',\n",
       " 'LA102190-0045_43.txt',\n",
       " 'WSJ910702-0078_1.txt',\n",
       " 'LA101289-0194_17.txt',\n",
       " 'WSJ880923-0163_113.txt',\n",
       " 'LA121189-0017_35.txt',\n",
       " 'LA103089-0043_55.txt',\n",
       " 'WSJ911121-0136_27.txt',\n",
       " 'LA010890-0031_71.txt',\n",
       " 'LA101289-0194_18.txt',\n",
       " 'WSJ910702-0078_34.txt',\n",
       " 'LA103089-0043_62.txt',\n",
       " 'WSJ910702-0078_32.txt',\n",
       " 'WSJ910107-0139_41.txt',\n",
       " 'SJMN91-06301029_54.txt',\n",
       " 'WSJ880923-0163_114.txt',\n",
       " 'LA103089-0043_66.txt',\n",
       " 'WSJ910702-0078_43.txt',\n",
       " 'LA061589-0143_32.txt',\n",
       " 'WSJ920114-0145_41.txt',\n",
       " 'WSJ911212-0080_4.txt',\n",
       " 'WSJ880621-0079_71.txt',\n",
       " 'LA103089-0043_65.txt',\n",
       " 'SJMN91-06184021_24.txt',\n",
       " 'LA011889-0067_20.txt',\n",
       " 'LA061589-0143_26.txt',\n",
       " 'LA121189-0017_38.txt',\n",
       " 'WSJ910702-0078_2.txt',\n",
       " 'LA061589-0143_27.txt',\n",
       " 'SJMN91-06012224_21.txt',\n",
       " 'LA052289-0050_67.txt',\n",
       " 'LA061589-0143_33.txt',\n",
       " 'SJMN91-06283083_38.txt',\n",
       " 'LA102190-0045_72.txt',\n",
       " 'WSJ911121-0136_20.txt',\n",
       " 'WSJ911121-0136_21.txt',\n",
       " 'WSJ910208-0130_7.txt',\n",
       " 'LA102190-0045_25.txt',\n",
       " 'SJMN91-06187248_65.txt',\n",
       " 'WSJ910326-0090_4.txt',\n",
       " 'LA103089-0043_56.txt',\n",
       " 'FBIS3-41_20.txt',\n",
       " 'LA040689-0056_23.txt',\n",
       " 'LA010890-0031_20.txt',\n",
       " 'SJMN91-06187248_72.txt',\n",
       " 'LA052289-0050_66.txt',\n",
       " 'LA051590-0065_9.txt',\n",
       " 'LA102190-0045_56.txt',\n",
       " 'LA121189-0017_42.txt',\n",
       " 'FT942-11114_34.txt',\n",
       " 'WSJ911121-0136_26.txt',\n",
       " 'SJMN91-06246065_33.txt',\n",
       " 'WSJ910702-0078_3.txt',\n",
       " 'LA042190-0060_3.txt',\n",
       " 'LA042790-0205_21.txt',\n",
       " 'WSJ910326-0090_5.txt',\n",
       " 'SJMN91-06187248_73.txt',\n",
       " 'LA032589-0044_25.txt',\n",
       " 'SJMN91-06276078_13.txt',\n",
       " 'SJMN91-06184088_23.txt',\n",
       " 'SJMN91-06246065_7.txt',\n",
       " 'SJMN91-06246065_8.txt',\n",
       " 'WSJ911121-0136_19.txt',\n",
       " 'WSJ910710-0148_13.txt',\n",
       " 'SJMN91-06184021_25.txt',\n",
       " 'LA102190-0045_24.txt',\n",
       " 'WSJ900705-0145_36.txt',\n",
       " 'SJMN91-06246065_37.txt',\n",
       " 'LA121189-0017_39.txt',\n",
       " 'SJMN91-06187248_68.txt',\n",
       " 'WSJ880621-0079_38.txt',\n",
       " 'LA042190-0060_2.txt',\n",
       " 'LA042790-0205_17.txt',\n",
       " 'WSJ911121-0136_16.txt',\n",
       " 'WSJ910702-0078_45.txt',\n",
       " 'LA081490-0030_17.txt',\n",
       " 'LA032589-0044_24.txt',\n",
       " 'LA032589-0044_32.txt',\n",
       " 'LA052289-0050_91.txt',\n",
       " 'SJMN91-06283083_39.txt',\n",
       " 'LA032589-0044_26.txt',\n",
       " 'SJMN91-06184021_23.txt',\n",
       " 'LA121189-0017_36.txt',\n",
       " 'LA061589-0143_23.txt',\n",
       " 'LA103089-0043_61.txt',\n",
       " 'SJMN91-06246065_36.txt',\n",
       " 'WSJ910326-0090_6.txt',\n",
       " 'LA010890-0031_21.txt',\n",
       " 'LA102190-0045_53.txt',\n",
       " 'LA102190-0045_51.txt',\n",
       " 'LA102190-0045_42.txt',\n",
       " 'SJMN91-06283083_28.txt',\n",
       " 'LA042790-0205_20.txt',\n",
       " 'LA102190-0045_35.txt',\n",
       " 'WSJ900705-0145_35.txt',\n",
       " 'SJMN91-06187248_66.txt',\n",
       " 'LA061589-0143_24.txt',\n",
       " 'SJMN91-06301029_53.txt',\n",
       " 'LA011889-0067_22.txt',\n",
       " 'LA061589-0143_30.txt',\n",
       " 'WSJ890828-0011_31.txt',\n",
       " 'LA052289-0050_92.txt',\n",
       " 'FT942-11114_33.txt']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "empty_files # these files dont have mat files generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'spurtWordTimes': array([[0.03, 0.11],\n",
      "       [0.11, 0.41],\n",
      "       [0.41, 0.56],\n",
      "       [0.56, 1.09],\n",
      "       [1.09, 1.18],\n",
      "       [1.18, 1.27],\n",
      "       [1.27, 1.67],\n",
      "       [1.67, 1.8 ],\n",
      "       [1.8 , 2.06],\n",
      "       [2.06, 2.68],\n",
      "       [2.68, 3.13],\n",
      "       [3.13, 3.34],\n",
      "       [3.34, 3.72],\n",
      "       [3.72, 4.09],\n",
      "       [4.09, 4.27],\n",
      "       [4.27, 4.4 ],\n",
      "       [4.4 , 4.69],\n",
      "       [4.69, 5.11],\n",
      "       [5.11, 5.31],\n",
      "       [5.31, 5.79],\n",
      "       [5.79, 5.91],\n",
      "       [5.91, 6.6 ],\n",
      "       [6.6 , 6.84],\n",
      "       [6.84, 7.04],\n",
      "       [7.04, 7.64]]), '__version__': '1.0', '__header__': 'MATLAB 5.0 MAT-file Platform: posix, Created on: Wed Dec  6 17:53:29 2023', 'words': array([u'the        ', u'ebrd       ', u'is         ', u'ministering',\n",
      "       u'to         ', u'a          ', u'region     ', u'that       ',\n",
      "       u'has        ', u'historical ', u'ties       ', u'with       ',\n",
      "       u'western    ', u'europe     ', u'and        ', u'that       ',\n",
      "       u'has        ', u'declared   ', u'its        ', u'commitment ',\n",
      "       u'to         ', u'democracy  ', u'and        ', u'free       ',\n",
      "       u'markets    '], dtype='<U11'), '__globals__': []}\n"
     ]
    }
   ],
   "source": [
    "# testing\n",
    "test = scipy.io.loadmat('../mat_files/words/FT911-2650_62_words.mat')\n",
    "print(test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py27",
   "language": "python",
   "name": "py27"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
