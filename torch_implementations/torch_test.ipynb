{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pprint as pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16])\n",
      "tensor(1.3575)\n",
      "tensor(0.0130)\n",
      "tensor(0.0982)\n",
      "tensor(-0.0531)\n",
      "tensor(-0.8901)\n",
      "tensor(-1.4857)\n",
      "tensor(-0.0862)\n",
      "tensor(0.9330)\n",
      "tensor(2.0888)\n",
      "tensor(-1.4564)\n",
      "tensor(1.5044)\n",
      "tensor(-0.5402)\n",
      "tensor(-0.6334)\n",
      "tensor(0.4792)\n",
      "tensor(-1.4052)\n",
      "tensor(1.7122)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Assuming you have a tensor of size [16, 1, 12]\n",
    "tensor = torch.randn(16)\n",
    "print(tensor.shape)\n",
    "for i in range(16):\n",
    "    print(tensor[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Before masking': Parameter containing:\n",
      "tensor([[ 0.1795, -0.2665, -0.5580,  0.8341,  0.1870,  0.2783],\n",
      "        [ 2.0395, -0.0427,  0.6600, -1.0849, -0.0823, -1.1895],\n",
      "        [ 1.3630,  1.1436,  0.8650,  0.6246, -0.2393,  0.4165],\n",
      "        [-0.1855,  1.3889,  0.1607,  1.2954,  1.1938,  0.2196],\n",
      "        [ 0.4168, -0.1767,  0.4800, -0.3006,  0.9134, -1.0233],\n",
      "        [-0.3070, -0.3941, -1.1438,  0.5393, -1.4720,  0.2769]],\n",
      "       requires_grad=True)}\n",
      "tensor([[ 0.1795, -0.2665, -0.5580,  0.8341,  0.1870,  0.2783],\n",
      "        [ 2.0395, -0.0427,  0.6600, -1.0849, -0.0823, -1.1895],\n",
      "        [ 1.3630,  1.1436,  0.8650,  0.6246, -0.2393,  0.4165],\n",
      "        [-0.1855,  1.3889,  0.1607,  1.2954,  1.1938,  0.2196],\n",
      "        [ 0.4168, -0.1767,  0.4800, -0.3006,  0.9134, -1.0233],\n",
      "        [-0.3070, -0.3941, -1.1438,  0.5393, -1.4720,  0.2769]],\n",
      "       grad_fn=<MulBackward0>)\n",
      "tensor([[ 0.1795, -0.0000, -0.5580,  0.8341,  0.1870,  0.0000],\n",
      "        [ 2.0395, -0.0000,  0.6600, -1.0849, -0.0823, -0.0000],\n",
      "        [ 1.3630,  0.0000,  0.8650,  0.6246, -0.2393,  0.0000],\n",
      "        [-0.1855,  0.0000,  0.1607,  1.2954,  1.1938,  0.0000],\n",
      "        [ 0.4168, -0.0000,  0.4800, -0.3006,  0.9134, -0.0000],\n",
      "        [-0.3070, -0.0000, -1.1438,  0.5393, -1.4720,  0.0000]],\n",
      "       grad_fn=<MulBackward0>)\n",
      "tensor([[ 0.1795, -0.0000, -0.5580,  0.0000,  0.1870,  0.0000],\n",
      "        [ 2.0395, -0.0000,  0.6600, -0.0000, -0.0823, -0.0000],\n",
      "        [ 1.3630,  0.0000,  0.8650,  0.0000, -0.2393,  0.0000],\n",
      "        [-0.1855,  0.0000,  0.1607,  0.0000,  1.1938,  0.0000],\n",
      "        [ 0.4168, -0.0000,  0.4800, -0.0000,  0.9134, -0.0000],\n",
      "        [-0.3070, -0.0000, -1.1438,  0.0000, -1.4720,  0.0000]],\n",
      "       grad_fn=<MulBackward0>)\n",
      "tensor([[ 0.1795, -0.0000, -0.5580,  0.0000,  0.1870,  0.2783],\n",
      "        [ 2.0395, -0.0000,  0.6600, -0.0000, -0.0823, -1.1895],\n",
      "        [ 1.3630,  0.0000,  0.8650,  0.0000, -0.2393,  0.4165],\n",
      "        [-0.1855,  0.0000,  0.1607,  0.0000,  1.1938,  0.2196],\n",
      "        [ 0.4168, -0.0000,  0.4800, -0.0000,  0.9134, -1.0233],\n",
      "        [-0.3070, -0.0000, -1.1438,  0.0000, -1.4720,  0.2769]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "## testing weight matrix\n",
    "hidden_size =6\n",
    "W = nn.Parameter(torch.randn(hidden_size, hidden_size))\n",
    "timesteps = [1,2,3,4]\n",
    "clockrates = [1,3,2,5]\n",
    "def masking_function(hidden_size,clock_val):\n",
    "    mask = []\n",
    "    for i in range(hidden_size):\n",
    "        if i%2==0 or i%clock_val == 0:\n",
    "            mask.append(1) \n",
    "        else : mask.append(0)\n",
    "\n",
    "    return torch.FloatTensor(mask)\n",
    "\n",
    "\n",
    "pp.pprint({\"Before masking\": W})\n",
    "for i,timestep in enumerate(timesteps):\n",
    "    mask = masking_function(hidden_size,clockrates[i])\n",
    "    pp.pprint(W*mask)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import pprint as pp\n",
    "def masking_function(hidden_size,clock_val):\n",
    "    mask = []\n",
    "    for i in range(hidden_size):\n",
    "        if i%2==0 or i%clock_val == 0:\n",
    "            mask.append(1) \n",
    "        else : mask.append(0)\n",
    "\n",
    "    return torch.FloatTensor(mask)\n",
    "\n",
    "mask = masking_function(12,2)\n",
    "print(mask[0].dtype)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.0000e+00, -8.7423e-08])\n",
      "tensor([4.0739, 3.0899])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "only one element tensors can be converted to Python scalars",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m pp\u001b[39m.\u001b[39mpprint(input_sequence)\n\u001b[1;32m      9\u001b[0m pp\u001b[39m.\u001b[39mpprint(clock_vals)\n\u001b[0;32m---> 10\u001b[0m input_sequence \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mtensor([input_sequence,clock_vals])\n\u001b[1;32m     11\u001b[0m input_sequence \u001b[39m=\u001b[39m input_sequence\u001b[39m.\u001b[39munsqueeze(\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39munsqueeze(\u001b[39m1\u001b[39m)\n\u001b[1;32m     12\u001b[0m \u001b[39mprint\u001b[39m(input_sequence)\n",
      "\u001b[0;31mValueError\u001b[0m: only one element tensors can be converted to Python scalars"
     ]
    }
   ],
   "source": [
    "import random\n",
    "def generate_sinusoidal_data(num_points, freq=1, amplitude=1):\n",
    "    t = torch.arange(0, num_points, 1)\n",
    "    x = amplitude * torch.sin(2 * torch.pi * freq * t / num_points)\n",
    "    return x\n",
    "input_sequence = generate_sinusoidal_data(2)\n",
    "clock_vals = torch.tensor([random.uniform(1, 6) for _ in range(2)])\n",
    "pp.pprint(input_sequence)\n",
    "pp.pprint(clock_vals)\n",
    "input_sequence = torch.tensor([input_sequence,clock_vals])\n",
    "input_sequence = input_sequence.unsqueeze(1).unsqueeze(1)\n",
    "print(input_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.)\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "def generate_non_repeating_subset(start, end, subset_size):\n",
    "    if subset_size > (end - start):\n",
    "        raise ValueError(\"Subset size cannot be greater than the range size.\")\n",
    "\n",
    "    numbers = list(range(start, end))\n",
    "    random.shuffle(numbers)\n",
    "    subset = numbers[:subset_size]\n",
    "    return subset\n",
    "\n",
    "import torch\n",
    "\n",
    "tst = torch.zeros(4)\n",
    "print(tst[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## copy of the torch chive architecture\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.nn.utils.rnn import pack_sequence, unpack_sequence\n",
    "\n",
    "class CHIVE(nn.Module):\n",
    "    def __init__(self, latent_space_dim):\n",
    "        super(CHIVE, self).__init__()\n",
    "        self.latent_space_dim = latent_space_dim\n",
    "        self.encoder = None\n",
    "        self.model_input = None\n",
    "        self.frnn_shape = 3\n",
    "        self.phrnn_shape = 3\n",
    "        self.sylrnn_shape = 3\n",
    "        self.input_shape = [self.frnn_shape, self.phrnn_shape, self.sylrnn_shape]\n",
    "\n",
    "        self._build()\n",
    "\n",
    "    def _build(self):\n",
    "        self._build_encoder()\n",
    "\n",
    "    def _build_encoder(self):\n",
    "        self.frame_rate_rnn = nn.LSTM(input_size=self.frnn_shape, hidden_size=64, batch_first=True)\n",
    "        self.phone_rate_rnn= nn.LSTM(input_size=self.phrnn_shape, hidden_size=64, batch_first=True)\n",
    "        self.syllable_rate_rnn = nn.LSTM(input_size=self.sylrnn_shape, hidden_size=64, batch_first=True)\n",
    "        # # self.encoder_input = [self.frame_rate_rnn_input, self.phone_rate_rnn_input, self.syllable_rate_rnn_input]\n",
    "\n",
    "        # # frame_rate_rnn = self.add_rnn_layer(self.frame_rate_rnn_input, self.frnn_shape)\n",
    "        # # phone_rate_rnn = self.add_rnn_layer(self.phone_rate_rnn_input, self.phrnn_shape)\n",
    "\n",
    "        # merged_layer = torch.cat([self.frame_rate_rnn[:,-1,:],self.phone_rate_rnn_input, self.syllable_rate_rnn_input], dim=1)\n",
    "        # syllable_rate_rnn = self.add_rnn_layer(merged_layer.unsqueeze(1), self.sylrnn_shape)\n",
    "\n",
    "        # bottleneck = self._add_bottleneck(syllable_rate_rnn)\n",
    "\n",
    "        # self.model_input = self.encoder_input\n",
    "        # self.encoder = nn.ModuleList([self.frame_rate_rnn_input, self.phone_rate_rnn_input, self.syllable_rate_rnn_input, bottleneck])\n",
    "        pass\n",
    "\n",
    "    def summary(self):\n",
    "        print(self.encoder)\n",
    "\n",
    "    def compile(self, learning_rate=0.0001):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
    "        self.loss_function = nn.MSELoss()\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "    def train(self, x_train, y_train, batch_size, num_epochs):\n",
    "        frnn_train, phrnn_train, sylrnn_train = x_train\n",
    "        frnn_train = torch.tensor(frnn_train, dtype=torch.float32)\n",
    "        phrnn_train = torch.tensor(phrnn_train, dtype=torch.float32)\n",
    "        sylrnn_train = torch.tensor(sylrnn_train, dtype=torch.float32)\n",
    "        y_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "\n",
    "        dataset = torch.utils.data.TensorDataset(frnn_train, phrnn_train, sylrnn_train, y_train)\n",
    "        train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            for data in train_loader:\n",
    "                frnn_batch, phrnn_batch, sylrnn_batch, y_batch = data\n",
    "                self.optimizer.zero_grad()\n",
    "                output = self.forward([frnn_batch, phrnn_batch, sylrnn_batch])\n",
    "                loss = self.loss_function(output, y_batch)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.frame_rate_rnn, self.phone_rate_rnn, self.syllable_rate_rnn = x\n",
    "        merged_layer = torch.cat([self.frame_rate_rnn[:, -1, :], self.phone_rate_rnn[:, -1, :], self.syllable_rate_rnn[:, -1, :]], dim=1)\n",
    "        return self._add_bottleneck(merged_layer.unsqueeze(1))\n",
    "\n",
    "    def _add_bottleneck(self, x):\n",
    "        lstm_units = 1\n",
    "        x, _ = nn.LSTM(input_size=x.size(-1), hidden_size=lstm_units, batch_first=True)(x)\n",
    "        return x[:, -1, :]\n",
    "\n",
    "    def add_rnn_layer(self, layer_input, shape):\n",
    "        lstm_units = 64\n",
    "        x, _ = nn.LSTM(input_size=shape, hidden_size=lstm_units,batch_first=True)(layer_input)\n",
    "        return x\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    chive = CHIVE(latent_space_dim=1)\n",
    "    chive.summary()\n",
    "\n",
    "    num_samples = 1000\n",
    "    frnn_sequence_length = 2\n",
    "    phrnn_sequence_length = 3\n",
    "    sylrnn_sequence_length = 3\n",
    "\n",
    "    frnn_data = np.random.rand(num_samples, frnn_sequence_length, 2)\n",
    "    phrnn_data = np.random.rand(num_samples, phrnn_sequence_length, 3)\n",
    "    sylrnn_data = np.random.rand(num_samples, sylrnn_sequence_length, 3)\n",
    "    print(frnn_data)\n",
    "    chive.compile()\n",
    "\n",
    "    dummy_targets = np.random.rand(800, 1)\n",
    "    chive.train(x_train=[frnn_data, phrnn_data, sylrnn_data], y_train=dummy_targets, batch_size=16, num_epochs=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6736, 0.6517, 0.6471, 0.9593, 0.3582, 0.9986, 0.2318, 0.0977, 0.4799,\n",
      "         0.5009, 0.9110, 0.5143, 0.7781]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "num_samples = 1\n",
    "\n",
    "test = torch.tensor(np.random.rand(1,1, 13))\n",
    "print(test[0])\n",
    "\n",
    "# def generate_non_repeating_subset(start, end, subset_size):\n",
    "#     if subset_size > (end - start):\n",
    "#         raise ValueError(\"Subset size cannot be greater than the range size.\")\n",
    "\n",
    "#     print(start, end, subset_size)\n",
    "#     numbers = list(range(start, end))\n",
    "#     random.shuffle(numbers)\n",
    "#     print(numbers[:8])\n",
    "#     subset = numbers[:subset_size]\n",
    "#     return sorted(subset)\n",
    "\n",
    "# test = generate_non_repeating_subset(0,int(num_samples), int(num_samples/2-1))\n",
    "# print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.0, 5.0, 8.0, 3.3333333333333335]\n"
     ]
    }
   ],
   "source": [
    "def average_reduce(input_list, target_length):\n",
    "    original_length = len(input_list)\n",
    "    step = original_length // target_length\n",
    "\n",
    "    reduced_list = [sum(input_list[i:i+step]) / step for i in range(0, original_length, step)]\n",
    "\n",
    "    return reduced_list[:target_length]\n",
    "\n",
    "# Example usage\n",
    "original_list = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]  # Your list of 4000 inputs\n",
    "target_length = 3  # Set the desired length\n",
    "\n",
    "reduced_list = average_reduce(original_list, target_length)\n",
    "print(reduced_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6.5, 6.5]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def average_reduce(input_list, target_length):\n",
    "    original_length = len(input_list)\n",
    "    dimension = len(input_list[0])  # Assuming all elements have the same dimensionality\n",
    "    step = original_length // target_length\n",
    "\n",
    "    reduced_list = [\n",
    "        np.mean(input_list[i]) for i in range(0, original_length, step)\n",
    "    ]\n",
    "\n",
    "    return reduced_list[:target_length]\n",
    "\n",
    "# Example usage\n",
    "original_list = [\n",
    "    np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]),\n",
    "    np.array([13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24]),\n",
    "        np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]),\n",
    "    np.array([13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24]),\n",
    "        np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]),\n",
    "    np.array([13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24]),\n",
    "        np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]),\n",
    "    np.array([13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24])]  # Your list of 4000 12-dimensional inputs\n",
    "target_length = 2  # Set the desired length\n",
    "\n",
    "reduced_list = average_reduce(original_list, target_length)\n",
    "print(reduced_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([ 7.,  8.,  9., 10., 11., 12., 13., 14., 15., 16., 17., 18.]), array([ 7.,  8.,  9., 10., 11., 12., 13., 14., 15., 16., 17., 18.])]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def average_reduce(input_list, target_length):\n",
    "    original_length = len(input_list)\n",
    "    dimension = len(input_list[0])  # Assuming all elements have the same dimensionality\n",
    "    step = original_length // target_length\n",
    "\n",
    "    reduced_list = [\n",
    "        np.mean(input_list[i:i+step], axis=0) for i in range(0, original_length, step)\n",
    "    ]\n",
    "\n",
    "    return reduced_list[:target_length]\n",
    "\n",
    "# Example usage\n",
    "original_list = [\n",
    "    np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]),\n",
    "    np.array([13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24]),\n",
    "        np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]),\n",
    "    np.array([13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24]),\n",
    "        np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]),\n",
    "    np.array([13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24]),\n",
    "        np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]),\n",
    "    np.array([13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24])]  # Your list of 4000 12-dimensional inputs\n",
    "target_length = 2  # Set the desired length\n",
    "\n",
    "reduced_list = average_reduce(original_list, target_length)\n",
    "print(reduced_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "for i in range(10): print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0718229331423694, 0.2768524715914172, -0.3074497367075231, -0.3874152872106194, -0.37507351903352437, 1.6532363760661308, 3.0573330039092186]\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "test = [7.521817800171207, 5.804137186790644, 3.963087713555305, 3.4533886165680245, 3.610095469736826, 3.5493848392085625, 3.5721557890435425, 3.6098015516283084, 3.724897184036044]\n",
    "\n",
    "rc= []\n",
    "\n",
    "for i in range(len(test)):\n",
    "    if i==0 : continue\n",
    "    if i == len(test)-1:break\n",
    "    rc_val = (test[i] - test[i+1])/(test[i-1] - test[i])\n",
    "    rc.append(rc_val)\n",
    "print(rc)    \n",
    "print(len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchaudio\n",
      "  Downloading torchaudio-2.1.1-cp311-cp311-manylinux1_x86_64.whl.metadata (6.4 kB)\n",
      "Collecting torch==2.1.1 (from torchaudio)\n",
      "  Downloading torch-2.1.1-cp311-cp311-manylinux1_x86_64.whl.metadata (25 kB)\n",
      "Requirement already satisfied: filelock in /home/gitpod/.pyenv/versions/3.11.5/lib/python3.11/site-packages (from torch==2.1.1->torchaudio) (3.12.4)\n",
      "Requirement already satisfied: typing-extensions in /workspace/.pyenv_mirror/user/current/lib/python3.11/site-packages (from torch==2.1.1->torchaudio) (4.5.0)\n",
      "Requirement already satisfied: sympy in /workspace/.pyenv_mirror/user/current/lib/python3.11/site-packages (from torch==2.1.1->torchaudio) (1.12)\n",
      "Requirement already satisfied: networkx in /workspace/.pyenv_mirror/user/current/lib/python3.11/site-packages (from torch==2.1.1->torchaudio) (3.1)\n",
      "Requirement already satisfied: jinja2 in /home/gitpod/.pyenv/versions/3.11.5/lib/python3.11/site-packages (from torch==2.1.1->torchaudio) (3.1.2)\n",
      "Collecting fsspec (from torch==2.1.1->torchaudio)\n",
      "  Downloading fsspec-2023.12.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.1.1->torchaudio)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m79.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.1.1->torchaudio)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m117.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.1.1->torchaudio)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m81.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.1.1->torchaudio)\n",
      "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.1.1->torchaudio)\n",
      "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.1.1->torchaudio)\n",
      "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch==2.1.1->torchaudio)\n",
      "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m31.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.1.1->torchaudio)\n",
      "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.1.1->torchaudio)\n",
      "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nccl-cu12==2.18.1 (from torch==2.1.1->torchaudio)\n",
      "  Downloading nvidia_nccl_cu12-2.18.1-py3-none-manylinux1_x86_64.whl (209.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.8/209.8 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch==2.1.1->torchaudio)\n",
      "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m40.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting triton==2.1.0 (from torch==2.1.1->torchaudio)\n",
      "  Downloading triton-2.1.0-0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.3 kB)\n",
      "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch==2.1.1->torchaudio)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.3.101-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/gitpod/.pyenv/versions/3.11.5/lib/python3.11/site-packages (from jinja2->torch==2.1.1->torchaudio) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /workspace/.pyenv_mirror/user/current/lib/python3.11/site-packages (from sympy->torch==2.1.1->torchaudio) (1.3.0)\n",
      "Downloading torchaudio-2.1.1-cp311-cp311-manylinux1_x86_64.whl (3.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m79.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading torch-2.1.1-cp311-cp311-manylinux1_x86_64.whl (670.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m670.2/670.2 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading triton-2.1.0-0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.2/89.2 MB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hDownloading fsspec-2023.12.0-py3-none-any.whl (168 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.9/168.9 kB\u001b[0m \u001b[31m28.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.3.101-py3-none-manylinux1_x86_64.whl (20.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.5/20.5 MB\u001b[0m \u001b[31m70.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: triton, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, fsspec, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch, torchaudio\n",
      "  Attempting uninstall: triton\n",
      "    Found existing installation: triton 2.0.0\n",
      "    Uninstalling triton-2.0.0:\n",
      "      Successfully uninstalled triton-2.0.0\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.0.1\n",
      "    Uninstalling torch-2.0.1:\n",
      "      Successfully uninstalled torch-2.0.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchvision 0.15.2 requires torch==2.0.1, but you have torch 2.1.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed fsspec-2023.12.0 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.18.1 nvidia-nvjitlink-cu12-12.3.101 nvidia-nvtx-cu12-12.1.105 torch-2.1.1 torchaudio-2.1.1 triton-2.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "libcudart.so.12: cannot open shared object file: No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorchaudio\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorchaudio\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfunctional\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mF\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorchaudio\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtransforms\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mT\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.5/lib/python3.11/site-packages/torchaudio/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m (  \u001b[39m# noqa: F401\u001b[39;00m\n\u001b[1;32m      2\u001b[0m     _extension,\n\u001b[1;32m      3\u001b[0m     compliance,\n\u001b[1;32m      4\u001b[0m     datasets,\n\u001b[1;32m      5\u001b[0m     functional,\n\u001b[1;32m      6\u001b[0m     io,\n\u001b[1;32m      7\u001b[0m     kaldi_io,\n\u001b[1;32m      8\u001b[0m     models,\n\u001b[1;32m      9\u001b[0m     pipelines,\n\u001b[1;32m     10\u001b[0m     sox_effects,\n\u001b[1;32m     11\u001b[0m     transforms,\n\u001b[1;32m     12\u001b[0m     utils,\n\u001b[1;32m     13\u001b[0m )\n\u001b[1;32m     14\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_backend\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcommon\u001b[39;00m \u001b[39mimport\u001b[39;00m AudioMetaData  \u001b[39m# noqa\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.5/lib/python3.11/site-packages/torchaudio/_extension/__init__.py:45\u001b[0m\n\u001b[1;32m     43\u001b[0m _IS_ALIGN_AVAILABLE \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[39mif\u001b[39;00m _IS_TORCHAUDIO_EXT_AVAILABLE:\n\u001b[0;32m---> 45\u001b[0m     _load_lib(\u001b[39m\"\u001b[39;49m\u001b[39mlibtorchaudio\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     47\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mtorchaudio\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_torchaudio\u001b[39;00m  \u001b[39m# noqa\u001b[39;00m\n\u001b[1;32m     49\u001b[0m     _check_cuda_version()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.5/lib/python3.11/site-packages/torchaudio/_extension/utils.py:64\u001b[0m, in \u001b[0;36m_load_lib\u001b[0;34m(lib)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m path\u001b[39m.\u001b[39mexists():\n\u001b[1;32m     63\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m---> 64\u001b[0m torch\u001b[39m.\u001b[39;49mops\u001b[39m.\u001b[39;49mload_library(path)\n\u001b[1;32m     65\u001b[0m torch\u001b[39m.\u001b[39mclasses\u001b[39m.\u001b[39mload_library(path)\n\u001b[1;32m     66\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/workspace/.pyenv_mirror/user/current/lib/python3.11/site-packages/torch/_ops.py:643\u001b[0m, in \u001b[0;36mload_library\u001b[0;34m(self, path)\u001b[0m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.5/lib/python3.11/ctypes/__init__.py:376\u001b[0m, in \u001b[0;36mCDLL.__init__\u001b[0;34m(self, name, mode, handle, use_errno, use_last_error, winmode)\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_FuncPtr \u001b[39m=\u001b[39m _FuncPtr\n\u001b[1;32m    375\u001b[0m \u001b[39mif\u001b[39;00m handle \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 376\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_handle \u001b[39m=\u001b[39m _dlopen(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_name, mode)\n\u001b[1;32m    377\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    378\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_handle \u001b[39m=\u001b[39m handle\n",
      "\u001b[0;31mOSError\u001b[0m: libcudart.so.12: cannot open shared object file: No such file or directory"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import torchaudio.functional as F\n",
    "import torchaudio.transforms as T\n",
    "from torchaudio.utils import download_asset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'download_asset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m SAMPLE_SPEECH \u001b[39m=\u001b[39m download_asset(\u001b[39m\"\u001b[39m\u001b[39m../Data_prep/data/aud1.wav\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'download_asset' is not defined"
     ]
    }
   ],
   "source": [
    "SAMPLE_SPEECH = download_asset(\"../Data_prep/data/aud1.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
